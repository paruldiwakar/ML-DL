# -*- coding: utf-8 -*-
"""ImageCaptioning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18FkUISfKNwXjIycG61osEFIfAM8_KVLN

#Image Captioning
##Steps
- Data collection
- Understanding the data
- Data cleaning
- Loading the training set
- Data Preprocessing - Images (img --> vectors)
- Data Preprocessing - Captions (words --> vectors)
- Word Embeddings
- Model Architecture
- Inference
"""

import numpy as np
import matplotlib.pyplot as plt
import time
import pickle
from keras.utils import to_categorical

import json

from google.colab import drive 
drive.mount('/content/drive')

!unzip -uq "/content/drive/My Drive/Colab Notebooks/flicker8k-dataset.zip"

def readTextFile(path):

  with open(path) as f:
    captions = f.read()
  return captions

captions = readTextFile("/content/flicker8k-dataset/Flickr8k_text/Flickr8k.token.txt")
captions = captions.split("\n")[:-1]
len(captions)

# dictionary to map each image with list of captions

description = {}

for x in captions:
    img_name,cap= x.split("\t")
    img_name = img_name.split(".")[0] 
    
    # if the image name is not already present
    if description.get(img_name) is None:
        description[img_name] = []

    description[img_name].append(cap)

description['1119015538_e8e796281e']

img_path = "/content/flicker8k-dataset/Flicker8k_Dataset/"

import cv2

img = cv2.imread(img_path +"1119015538_e8e796281e" +".jpg") 
plt.imshow(img)
plt.show()

"""## Data Cleaning"""

"""
1. lower case             # dont remove stopwards   
2. remove punctuations    # dont do stemming
3. remove len(words)<2
4. can remove special symbols like @,<,# etc
"""
import re

def clean_text(sample):
    sample = sample.lower()
    """
    substitute anything not[^] between a-b with space " " and ,
    + for 2 or more occurances of the preceeding character 
    """
    sample = re.sub("[^a-z]+"," ",sample) 
    sample = sample.split(" ")
    sample = [s for s in sample if len(s) > 1]
    sample = (" ").join(sample)
    return sample

# Modifying and cleaninig
for key,cap_list in description.items():
    for i in range(len(cap_list)):
        cap_list[i] = clean_text(cap_list[i])

#description['1119015538_e8e796281e']

with open("descriptions.txt","w") as f:
  f.write(str(description))

"""## Vocabulary"""

descriptions = None
with open("descriptions.txt","r") as f:
  descriptions = f.read()
json_acceptable_string = descriptions.replace("'","\"")
descriptions = json.loads(json_acceptable_string)

print(type(descriptions))

vocabulary = set()

for key in descriptions.keys():
    [vocabulary.update(sent.split()) for sent in descriptions[key]]

print("Vocab size : %d"%len(vocabulary))

# All word in descriptions
total_words = []

for key in descriptions.keys():
  #des is one caption in list of captions and i is one word in that caption
  [total_words.append(i) for des in descriptions[key] for i in des.split()]
print("Total words : %d"%len(total_words))

# Filter words from the vocab according to certain threshold frquency

from collections import Counter

counter = Counter(total_words)
freq_cnt = dict(counter)
len(freq_cnt.keys())

# sort the dictionary in descending order of frequency
sorted_freq_cnt = sorted(freq_cnt.items(), key= lambda x: x[1],reverse=True)

# Filter
threshold  = 10
sorted_freq_cnt  = [x for x in sorted_freq_cnt if x[1]>threshold]
total_words = [x[0] for x in sorted_freq_cnt]
#total_words

"""## Prepare Train/Test Data"""

train_file_data = readTextFile("/content/flicker8k-dataset/Flickr8k_text/Flickr_8k.trainImages.txt")
test_file_data = readTextFile("/content/flicker8k-dataset/Flickr8k_text/Flickr_8k.testImages.txt")

train = [e[:-4] for e in train_file_data.split('\n')[:-1]]
test = [e[:-4] for e in test_file_data.split('\n')[:-1]]

#train

# Prepare Descriptions for Training data
# Tweak - Add a <s> and <e> token to our training data

train_descriptions = {}

for img_id in train:
    train_descriptions[img_id] = []
    for cap in descriptions[img_id]:
        cap_to_append = "<s> "+cap+" <e>"
        train_descriptions[img_id].append(cap_to_append)

#train_descriptions['2752809449_632cd991b3']

"""# Data Preprocessing - Images

## Transfer Learning
- Images --> Features
- Text --> Features

##Step - 1 Image Feature Extraction
"""

import tensorflow as tf

tf.compat.v1.disable_eager_execution()

from tensorflow.python.keras.layers import *
from tensorflow.python.keras.layers.advanced_activations import LeakyReLU
from tensorflow.python.keras.models import Sequential,Model
from tensorflow.compat.v1.keras.optimizers import Adam
from keras.preprocessing import image
from keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input

model = ResNet50(weights='imagenet',input_shape=(224,224,3))
model.summary()

new_model = Model(inputs = model.input,outputs = model.layers[-2].output)
new_model.summary()

def preprocess_image(img):
  img = image.load_img(img,target_size=(224,224))
  img = image.img_to_array(img)
  img = np.expand_dims(img,axis=0)
  img = preprocess_input(img) #normalises image array
  return img

'''
img = preprocess_image(img_path+"1119015538_e8e796281e.jpg")
plt.title(img.shape)
plt.imshow(img[0])
plt.show()
'''

def encode_image(img):
  img = preprocess_image(img)
  fea_vec = new_model.predict(img) # -->(1,2048)
  fea_vec = fea_vec.reshape((fea_vec.shape[1],)) # -->(2048,)
   
  return fea_vec

'''
sample: /content/flicker8k-dataset/Flicker8k_Dataset/1119015538_e8e796281e.jpg
'''

# img_id --> feature vector extracted from Resnet
start = time.time()

# key is file name and value is 2048 feature vectors
encoding_train = {}


for ix,img_id in enumerate(train):
  try:

    path = img_path+"/"+img_id+".jpg" 
    encoding_train[img_id] = encode_image(path)
  except:
    continue

  if ix%100 == 0:
    print("Encoding image :"+str(ix))

print("Time taken in sec :" + str(time.time()-start) )

# Store everything to the disk //

with open("./encoded_train_images.pkl",'wb') as f:
  pickle.dump(encoding_train,f)

# img_id --> feature vector extracted from Resnet
start = time.time()

# key is file name and value is 2048 feature vectors
encoding_test = {}


for ix,img_id in enumerate(test):
  try:

    path = img_path+"/"+img_id+".jpg" 
    encoding_test[img_id] = encode_image(path)
  except:
    continue

  if ix%100 == 0:
    print("Encoding image :"+str(ix))

print("Time taken in sec :" + str(time.time()-start) )

# Store everything to the disk //
with open("./encoded_test_images.pkl",'wb') as f:
  pickle.dump(encoding_test,f)

with open("./encoded_train_images.pkl",'rb') as f:
  encoding_train = pickle.load(f)
 
with open("./encoded_test_images.pkl",'rb') as f:
  encoding_test = pickle.load(f)

"""## Data Preprocessing - Captions"""

len(total_words)



word_to_idx = {}
 idx_to_word = {}


 for i,word in enumerate(total_words):
   word_to_idx[word] = i+1
   idx_to_word[i+1] = word

word_to_idx['dog']

idx_to_word[6]

word_to_idx['<s>'] = 1846
word_to_idx['<e>'] = 1847
idx_to_word[1846] = '<s>'
idx_to_word[1847] = '<e>'

vocab_size = len(idx_to_word) + 1

print("Vocab size : %d"%vocab_size)

all_caption_len = []

for key in train_descriptions.keys():
  for cap in train_descriptions[key]:
    all_caption_len.append(len(cap.split()))

max_len = max(all_caption_len) 
max_len

with open("./word_to_idx.pkl","wb") as w2i:
    pickle.dump(word_to_idx,w2i)

with open("./idx_to_word.pkl","wb") as i2w:
    pickle.dump(idx_to_word,i2w)

"""## Data Loader (Gen)"""



#wei wuxian lan wangji wei ying lan zhan

def data_generator(train_descriptions,encoding_train,word_to_idx,max_len,batch_size):

  X1,X2,y = [],[],[]
  n = 0

  while True:
    for key,desc_list in train_descriptions.items():
      n+=1
      photo = encoding_train[key] 

      for desc in desc_list:

        seq = [word_to_idx[word] for word in desc.split() if word in word_to_idx]

        for i in range(1,len(seq)):
          in_seq = seq[0:i]
          out_seq = seq[i]

          in_seq = pad_sequences([in_seq],maxlen=max_len,value=0,padding='post')[0]
          out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]

          X1.append(photo)
          X2.append(in_seq)
          y.append(out_seq)

      if n == batch_size:
        yield([np.array(X1),np.array(X2)],np.array(y))
        X1,X2,y = [],[],[]
        n=0

for i in data_generator(train_descriptions,encoding_train,word_to_idx,max_len,3):
  x,y = i
  print(y.shape)
  break

"""## Word Embeddings"""

f = open('/content/drive/My Drive/Colab Notebooks/glove.6B.50d.txt',encoding='utf-8')

embedding_index = {}

for line in f:
  values = line.split()
  word = values[0]
  word_embedding = np.asarray(values[1:],dtype='float')

  embedding_index[word] = word_embedding

f.close()

#embedding_index["apple"]

def get_embedding_matrix():

  emb_dim = 50
  embedding_output = np.zeros((vocab_size,emb_dim)) 
  
  for word,idx in word_to_idx.items(): 
     embedding_vector = embedding_index.get(word)
     
     if embedding_vector is not None:
         embedding_output[idx] = embedding_vector

  return embedding_output

embedding_matrix = get_embedding_matrix()
embedding_matrix.shape

"""## Model Architecture"""

# Image feature extraction

input_img_fea = Input(shape=(2048,))
inp_img1 = Dropout(0.3)(input_img_fea)
inp_img2 = Dense(256,activation='relu')(inp_img1)

# Partial caption sequence model

input_cap = Input(shape=(max_len,))
inp_cap1 = Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_cap)
inp_cap2 = Dropout(0.3)(inp_cap1)
inp_cap3 = LSTM(256)(inp_cap2)

'''
inp_img2(224x224 -> 2048 -> 256 dims)
inp_cap3(batch_sizex35 -> batch_sizex35x50 -> LSTM -> 256 dims)
inp_img2+inp_cap3 -> 256 dims -> 1847 dims -> softmax -> prob
'''
decoder1 = add([inp_img2,inp_cap3])
decoder2 = Dense(256,activation='relu')(decoder1) 
output = Dense(vocab_size,activation='softmax')(decoder2)

# Combined Model

model = Model(inputs= [input_img_fea,input_cap], outputs = output)

model.summary()

# Important - Embedding Layer
#(initialising the embedding layer in Model with predefined embedding matrix)

model.layers[2].set_weights([embedding_matrix])
model.layers[2].trainable = False

model.compile(loss='categorical_crossentropy',optimizer='adam')

"""## Training Model"""

epochs = 10
batch_size = 7
steps = len(train_descriptions)//12

for i in range(epochs):
  gen = data_generator(train_descriptions,encoding_train,word_to_idx,max_len,batch_size)
  model.fit_generator(gen,epochs=1,steps_per_epoch=steps,verbose=1)
  model.save("best_model.h5")

"""#Predictions"""

def predict_caption(photo):

  in_text = "<s>"
  for i in range(max_len):
    sequence = [word_to_idx[w] for w in in_text.split() if w in word_to_idx]
    sequence = pad_sequences([sequence],maxlen=max_len,padding='post')

    y_pred = model.predict([photo,sequence])
    y_pred = y_pred.argmax()
    word = idx_to_word[y_pred]
    in_text += (' '+ word)
    
    if word == '<e>':
      break

  final_caption = in_text[3:-3]  
  final_caption = ''.join(final_caption)

  return final_caption

# Pick some random images and see results

for i in range(15):
  idx = np.random.randint(0,1000)
  all_img_name = list(encoding_test.keys())
  img_name = all_img_name[idx]
  photo_2048 = encoding_test[img_name].reshape((1,2048))
  
  i = plt.imread(img_path+img_name+".jpg")
  caption = predict_caption(photo_2048)
  print(caption + ".")
  plt.imshow(i)
  plt.show()

